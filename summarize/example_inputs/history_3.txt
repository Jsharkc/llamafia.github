================================================================================
chat_session_len:38
用户1:Mistral v0.3在HuggingFace放出来了（没有细节），有人测过benchmark分数了吗？
用户9:才上传 8 小时 x
用户9:哪里有 v0.2 的 score 呀
用户9:虽然 MATH 单靠自然语言已经被刷到 80+ 了，但一般的model要正确率比较高还是要靠 code，请问有群友知道有什么 framework 执行这种自然语言+code夹杂的 generation pipeline 比较高效吗？
用户11:chain-of-symbol prompting
用户9:太强啦！我看看
用户9:但目前最好的应该还是类似 Code Interpreter 这种模式
用户4:我没get 😂
用户9:纯 code 就是程序输出作为答案，Code Interpreter 就是程序输出会加回 context 然后 LLM 继续 reasoning
用户9:感觉会涉及到很多异步？想请问熟悉vLLM的朋友，vLLM的异步实现相比一般的模式效率会有区别吗🤔
用户2:langchain可以做么？公司内部有一个自己的类langchain lib叫onetwo，做你这个需求很方便，但不知道third party版体验如何
用户9:稍微想了一下，感觉关键就是大量异步操作
用户9:自己鼓捣一下应该也不难
用户4:[破涕为笑] 你是说 如果是 python 的话 或许通过 streaming 来 async 的执行 python statements 吗 :P
用户9:想想好像也不复杂？但如果有现成的当然是最好啦
用户9:希望尽量把GPU和CPU都打满
用户2:onetwo就是把async包的很好，其他的没什么特别的，llm call和code interpreter都被抽象成engine/agent这种形式了，langchain如果也是这样的话应该也可以支持
用户2:我有体验过实现alphago那种形式的mcts，llm call和cpu computation都heavy的那种，写起来蛮舒服
用户9:可以的可以的
用户9:感谢解答！
用户9:感觉就是一些agent framework比较可能会有这种功能
用户6:因为llm走的是gpu，然后编译执行那块是cpu，可能想想优化的是让cpu和gpu都跑满吧
用户9:两边要相互等待
用户3:好像理解了，是有些优化空间，比如 LM 写完一句code就执行一句[旺柴]
用户3:我有个大胆的想法，用GPU执行Python
用户5:我以为是lm输出code后交付执行，若干cot step后再拿结果修正之前推理
用户9:嗯，但是如果想大规模搜索就比较有用了
用户3:起码省了IO[旺柴]
用户9:笑死[旺柴][旺柴]
用户10:going 2000
用户3:https://github.com/HigherOrderCO/Bend
其实现在有一些 PoC 是在干这个事的，未来可能所有计算都可以在 GPU 上完成了
用户3:普通计算里也存在并行的空间
用户9:[天啊][天啊]学到了
用户9:感觉async这部分是简单的
用户9:麻烦的是把code的输出加到context里，是不是会打乱vllm对kvcache的调度。印象里vllm目前还不支持中途插入context
用户3:sglang能不能派上用场
用户7:大佬们怎么看待这篇：Your Transformer is Secretly Linear
https://arxiv.org/abs/2405.12250
用户8:https://docs.google.com/forms/d/e/1FAIpQLSeL20BzIA0MT-8n3KnheQqEPQ448vVjXJh-shswidiymo1q8g/viewform 请大家积极参与neurips review [合十]

================================================================================
chat_session_len:42
用户13:https://papers.cool/arxiv/2405.14828
24年了，good seed 还是 all you need [捂脸]
用户6:@熊伟 UIUC/Google alignment agent 你们的 Iterative DPO 实现的时候是纯 offline 的嘛 先生成 然后走rm 然后 DPO 和 RS 一样?
用户1:你说的offline包括多次迭代rm吗
用户1:反正就是 生成response 用rm标一下 训练policy，循环几次，没有更新rm
用户6:就是 rm不是常驻内存 就离线标注
用户10:可以不常驻内存的
用户7:我感觉用 RM 打，其实不如人快速标一遍[捂脸]
用户7:反正都是彻底离线在跑了
用户1:llama和claude他们结合了用人和rm
用户1:用人还是贵吧
用户7:RM 感觉还是会有hacking 的问题，最典型的还是那个倾向长文本或某些 hacking phrase
用户1:hacking不光是rm的问题，其实我们测过BT rm的verbosity bias并没有大家想象中那么严重。一个几乎没有verbosity bias的rm在训练过程中也会明显变长。另外就是即使是ground truth rm也会有“hacking”，例如math即使有gt 答案，所以hacking更多是在optimization过程中导致的。rm跟人工标比还是能省不少人力
用户2:我们训练了很多Reward Models - 体感是现在高质量preference data不够多（覆盖的distribution不够广），所以经常OOD generation比较差 / 或者学到了一些spurious correlation（比如URL这种reward hacking）
用户3:URL是什么意思
用户3:只靠这么少的信号监督学到spurious correlation太正常了
用户2:感觉哪家公司能多放一些人类标注的preference data（比如HelpSteer这种），对这个领域的提升会很大
用户3:就跟用cnn识别狗的品种发现在识别背景颜色一样[旺柴]
用户3:需要high dimensional reward
用户3:需要对齐可能说明预训练的分布就是不对齐期望分布的，那么对齐是不是icing on the cake，只是选取一小部分分布调整其形状，终归是覆盖不完全的
用户5:所以就是离线的，分不同训练阶段
用户5:（如果我没遗漏代码细节，@初七-NVIDIA-LLM 
用户5:get，rm的训练数据处理，和ppo/dpo时候加惩罚，是目前比较常见的做法
用户1:嗯，我们试过数据比loss好用。尽管math上差不多😂
用户12:提问🙋 gpt4o 符合大家的预期吗
用户4:响应速度超出预期🤔
用户12:唔，还是看整体聪明程度
用户4:中文多模ocr比之前聪明
用户3:https://huggingface.co/spaces/Jellyfish042/UncheatableEval
压缩能力榜单
用户9:今天试了一下语音通话，好像实时打断的功能没了？
用户4:举个简单的例子[捂脸] 这样的图片给他ocr+reasoning
用户9:感觉中文的语音对话能力没有超过我的预期，首先实时语音对话好像无了，然后问题复杂了，推理时间好像会增加？
用户11:这个榜是RWKV发的
用户11:说用dolma准备更多的训练data……
用户14:dolma不太行
用户14:之前和EleutherAI的人说过，dolma不太行，可能是没听进去？
用户11:而且新数据和uncheatable没什么关系，略微调整下分布想让谁在上面就谁在上面吧[捂脸]
用户8:说到ocr，前几天测了一下，gemini advanced ocr latex的内容有点不太行，表格会容易读串行，有朋友知道这种用mmlm做ocr/table的benchmark么？感觉有点意思
用户8:gpt4o中文表格图片那识别有点惨
用户3:看起来gemini注意力有些涣散
用户8:看了一下串行串列读错数字都有(
用户8:(突然感觉也不能说multi-modal真的被解决了，就算是native support)
用户8:按我的理解，ocr这个task都做了好多年了我拿adobe的识别都不见得会做的这么差

================================================================================
chat_session_len:43
用户7:话说投票要不要加一个选做题，让大家把觉得有趣的case写上来🤔
用户7:[666]有道理
用户4:llamafia poll 成为继chatbot arena 后第二可用eval
用户7:[破涕为笑][捂脸]llamafia arena？…
用户7:话说4o体感有时候懒 有些case勤快 不知道大家有没有遇到🤔
用户7:参照是4v 4turbo
用户11:gpt4o推理速度很赞
用户11:要是能更快就更好了，快到每打一个字符都自动出答案那种
用户8:纯文本我发现逻辑推理能力比gpt-4-turbo强，deductive/abductive reasoning
用户8:我最近在搞一个logic reasoning的synthetic data，gpt-4o的performance在相对简单的reasoning题目上比gpt-4能高 10%，f1在80%左右，gpt-4-turbo是在70%左右
用户15:code和翻译都不太行
用户16:确实，试了一下之后还是选择claude写code
用户16:总不能以后Gemini搞long-context，claude搞code，其他情况gpt-4[旺柴]，三足鼎立
用户16:我搞visualization的code感觉sonnet理解我的intent的情况总比gpt4o/turbo多，写出来直接能跑看起来pleasant的概率也更高(我基本也是用ai来干这个事，我自己画图太差了
用户10:嗯嗯 肯定是在不同 coding domain 上有不同的 perf
用户12:请问有比较好的xai的paper reading list可以推荐的吗？
用户9:gemini1.5我感觉现在能用一点了 虽然复杂一点的指令还是去听不懂
用户9:当时速度好快 各种给它数据，画图画表格
用户16:现在确实比1.0好多了
用户16:1.0...哎也不知道为什么发布这个版本
用户14:有一个问题没想明白，既然同样效果的模型，moe训练和使用成本更低，同样成本训练moe效果更好，是不是大多数情况下都应该训练moe模型？为啥像llama等一直在搞dense模型，咋没有训练moe模型呢？
用户13:你们也说了是同样的active param, 那整个部署系统中的MoE多出的参数量也是算成本的, 在不同的部署场景里面, 它也不是完成胜于dense的. 所有的前提都在于你服务可以打到算力上限.
用户13:当然, llama的选择我同样抱有疑问, 只能说有可能他们在探上限
用户4:然后蒸馏吗[旺柴]
用户2:现在的moe基本上只能和两倍active params的模型效果差不多
用户2:我个人是感觉moe做了这么多年还是没做得很work
用户2:sparse优化的问题还是需要基础理论的突破
用户1:MoE从91年Michael Jordan和Hinton他们提出来已经三十多年了，架构其实一直没什么大变化
用户12:而且基本都是Google在solo
用户1:对，前些年主要是Google那边工程上把MoE给Scale up上去了
用户1:过去一年由GPT4引爆了MoE的研究新热潮 - 但我没看到什么理论上的新的切入点
用户2:https://arxiv.org/abs/2310.00811
用户12:感觉从理论上看，moe比较像是深层次/更复杂的model ensemble
用户6:想问一下群友，有没有一些文章实验证明了长序列训练确实能提升模型性能
用户6:有没有sequence维度的scaling law之类的[旺柴]
用户5:https://manifestai.com/articles/compute-optimal-context-size/
用户6:NB，我看一下
用户3:这个测试还挺有启发性的
用户3:不是简单的大海捞针
用户17:这个似乎和babilong有点像？
用户17:相当于把一个多跳推理题的n个condition拆开洒进去
用户4:像 Chameleon 这种pretrain阶段就 mixed-modal 的建模方式，可能会有什么单独 训练不同模态再 encode 输入语言模型的方式所不具备的能力？比如多模态的上下文学习，可以很轻松解决的人物一致性🤔
用户18:到目前为止贵群和 twitter 对于 gpt4o 的看法类似

================================================================================
chat_session_len:3
用户2:请问有百川的同学吗？可否帮忙联系一下这篇paper https://papers.cool/arxiv/2405.14591 的作者，对齐一下细节。我自己的数值计算结果不大对得上论文的结果
用户1:我联系你哈
用户2:好的，谢谢，已经联系上

================================================================================
chat_session_len:50
用户4:现在有视频理解模型能把里面不同技术和油耗的下降提取出来吗👀
用户17:冒昧请问下大家，有没有类似 BBH NI 的中文数据集推荐呢，我这边找到 COIG 这个数据集，但是感觉对于单条 instruction，这个数据集的 examples 不太多的？
用户13:我可以来解决下
用户7:请教一下，群里有大佬搞过H卡的FP8训练吗，稳定性和效率如何呀，之前跟NV的人交流说还比较难落地训练，不知道现在进展如何了
用户16:《Transformer升级之路：18、RoPE的底数设计原则》
https://kexue.fm/archives/10122

本文简单介绍了论文《Base of RoPE Bounds Context Length》，它从语义聚合的期望性质讨论了RoPE的底数下界，由此指出更大的训练长度应该选择更大的底数，而不单单是为了配合“先短后长”的训练策略、继而利用NTK-RoPE来降低初始损失的折中选择。

https://mp.weixin.qq.com/s/-PKvDf7HO82gr3tONY1-gQ
用户14:谢谢@苏剑林 的详细解读和分享，这个工作还有很多局限性，不过还是希望对RoPE的分析以及后续对长窗口上的扩展能有一些借鉴的意义。现在这个RoPE里面base的下界还是数值解，有兴趣的同学朋友可以试试能不能求出来解析解[呲牙]欢迎都与我们沟通哈
用户18:https://arxiv.org/abs/2405.16684#:~:text=Past%20work%20has%20established%20scaling,of%20a%20fixed%20compute%20budget.
用户18:data-dependent scaling law
用户15:提问：大家觉得 7b 的模型在两年之后，效果能 match 今天 100b 的模型吗 🤔
用户1:效果是指知识的储量还是回答问题的准确性
用户1:如果是后者，我觉得有可能
用户4:两年前的 GPT3 175b 和现在的 llama3 8b[旺柴]
用户4:知识的储量其实还是有很大的优化空间，无论是绝对容量还是有用知识的存量
用户10:如果成立，意味着未来做端侧模型的公司会很多，杨老师也算提前去踩路了
用户4:knowledge scaling law 里面谈到在不同domain的数据前面加区分domain的tag就能增加30%的知识容量，感觉这种low hanging fruit 可能还有很多
用户15:我感觉现在电脑起步 8G 完全是为了搞价格歧视弄钱
用户3:我想提个可能有点naive的问题，我看语言模型都是B级的，sam这种视觉模型只有几百M，是因为视觉数据信息量更少吗同样的scale下
用户15:我用 ollama 跑 7b 模型的时候，常驻内存是 16G 往上，于是我 24G 内存的 mac 就刚好能跑，但我比较难想象如果只有 16G 显存应该怎么跑端侧模型
用户5:这时候apple的统一内存架构就香起来了
用户5:话说我4090跑7B 大约是5-6g常驻 是因为🤔
用户5:默认量化吗
用户18:单纯的视觉模型不需要推理吧
用户12:一个不需要推理的判别模型和大生成模型 非常不一样
用户12:sora这种也是需要推理的生成模型 也是B级
用户18:生成模型为啥需要参数多
用户15:因为生成[破涕为笑]
用户18:我记得MAE里encoder比decoder大？
用户3:因为要学分布？
用户3:mae说了他们的decoder是lightweight
用户12:因为生成需要model uncertainty
用户15:在翻译和摘要这俩任务上，for a very long time 大的 encoder 被认为比 decoder 更重要的
用户15:最初代用 pretrained LM 做 summarization 的文章是在 bert 上面架一个 rnn 做 decoding https://arxiv.org/pdf/1903.10318
用户4:数据量越多，架构越简化也只能更简化
用户15:我的印象里是一直到 2022 下半年，大家才意识到当 decoder 大了之后，它本身 encode 的能力就跟 encoder 一样了；decoder 也能 encode 应该是 yet another 典型的小 scale 不成立，但大 scale 就成立的例子
用户3:Interesting
用户18:现在主流的VLM都是用SigLip这种特别小的模型encode
用户4:bpe 是一种encoder
用户18:sry，开源vlm
用户6:不太清楚现在一般大家都是怎么做的
用户6:bert时候可以拿［CLS］token当作input的representation，因为pretraining有相关的task
用户4:mixtral也搞代码模型了，Codestral
用户3:现在大家多节点训练用的比较多的架构是fsdp吗
用户5:小模型fsdp效率足够高
用户10:你觉得这种设备会有多大的出货量？有多少正常人会替换智能手机用ai手机？
用户11:encoding能力的测试还是挺抽象的，大概有两类，一类是token/sentence classification，一类是retrieval
用户11:reward model有scaling law吗
用户9:还是 openai 的
用户11:hh，刚刚也搜到了
用户18:or decoder only model
用户2:说明有额外训练的话，decoder only模型用last token的表示完全ok

================================================================================
